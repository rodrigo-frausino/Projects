{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= #0cc754> <h1 align = \"center\">Digit Recognition with Deep Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <font color= #0cc754> Table of CONTENTS </font>\n",
    "\n",
    "1. [Introduction](#1)\n",
    "2. [Library Imports](#2)\n",
    "3. [Reading and Preparing the Data](#3)\n",
    "4. [The Model](#4)\n",
    "    1. [Training the Model](#4.1)\n",
    "    2. [Evaluating the Model](#4.2)\n",
    "5. [Hyperparameter tuning the Model](#5)\n",
    "6. [Convolutional Neural Network](#6-convolutional-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## <font color= #0cc754> 1. Introduction </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an organized way for Project 3: Digit recognition (Part 2) of the Course I took online in 2024 from MITx called `Machine Learning with Python-From Linear Models to Deep Learning`. This notebook isn't a solution, but instead based on that project.\n",
    "\n",
    "Our goal is to implement a neural network to classify MNIST digits, a rather famous database of handwritten digits. We are going to use the PyTorch library for this purpose. We are going to Hyperparameter tune it using Bayesian Optimization. At the end we are going to use a Convolutional Neural Network (CNN) to do the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## <font color= #0cc754> 2. Library Imports </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## <font color= #0cc754> 3. Reading and Preparing the Data </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We need to load the data from the file mnist_data.pkl.gz and split it into training,  validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('./data/mnist_data.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "X_train, y_train = train_set\n",
    "X_valid, y_valid = valid_set\n",
    "X_test, y_test = test_set\n",
    "X_train=np.vstack((X_train,X_valid))\n",
    "y_train=np.hstack((y_train,y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To release memory and split the train dataset we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_set, valid_set, test_set, X_valid, y_valid\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2, random_state=42, \n",
    "                                                 shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 784)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert our arrays into Pytorch tensors and batchify it for better efficience in the training process of our Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "dev_x_tensor = torch.tensor(X_dev, dtype=torch.float32)\n",
    "dev_y_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
    "test_x_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "dev_dataset = TensorDataset(dev_x_tensor, dev_y_tensor)\n",
    "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## <font color= #0cc754> 4. The Model </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple Neural Network, so we have a reasonably fast training time. The output should be of size 10, since we are trying to predict 10 classes (the digits 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(784, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "        )\n",
    "lr=0.1\n",
    "momentum=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "### <font color= #0cc754> 4.1 Training the Model </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the Neural Network ( the model), computing the test accuracy and Loss. We, as well, keep track of the validation set accuracy and Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy of training\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    return running_loss / len(train_loader), correct_predictions / total_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_one_epoch(model, dev_loader, criterion):\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dev_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy of validation\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "        return running_loss / len(dev_loader), correct_predictions / total_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:12,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4765, Training Accuracy: 0.860 Validation Loss: 0.3157, Validation Accuracy: 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:11,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3133, Training Accuracy: 0.910 Validation Loss: 0.3239, Validation Accuracy: 0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:10,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2885, Training Accuracy: 0.916 Validation Loss: 0.2706, Validation Accuracy: 0.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:08,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2713, Training Accuracy: 0.921 Validation Loss: 0.2711, Validation Accuracy: 0.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:07<00:07,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2611, Training Accuracy: 0.925 Validation Loss: 0.2781, Validation Accuracy: 0.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:08<00:05,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2527, Training Accuracy: 0.926 Validation Loss: 0.2689, Validation Accuracy: 0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:10<00:04,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2393, Training Accuracy: 0.931 Validation Loss: 0.2424, Validation Accuracy: 0.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:11<00:02,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2281, Training Accuracy: 0.935 Validation Loss: 0.2443, Validation Accuracy: 0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2202, Training Accuracy: 0.936 Validation Loss: 0.2659, Validation Accuracy: 0.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2163, Training Accuracy: 0.937 Validation Loss: 0.2399, Validation Accuracy: 0.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss = evaluate_one_epoch(model, dev_loader, criterion)\n",
    "    print(f\"Training Loss: {train_loss[0]:.4f}, Training Accuracy: {train_loss[1]:.3f}\"\n",
    "          + f\" Validation Loss: {val_loss[0]:.4f}, Validation Accuracy: {val_loss[1]:.3f}\" )\n",
    "\n",
    "# Let's save the model    \n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "### <font color= #0cc754> 4.2 Evaluating the Model </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to evaluate our trained model in the test set. Calculating the Loss and the Accuracy of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2371, Test Accuracy: 0.931\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_one_epoch(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss[0]:.4f}, Test Accuracy: {test_loss[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## <font color= #0cc754> 5. Hyperparameter tuning the Model </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be fair a Deep Learning model has MANY hyperparameters we can tune. The number of hidden layers, the number of neurons in each hidden layer, which optimizer we can use, learning rate, momentum, the architeture itself of our model can be drastically changed and so and so on. We are going to optimize by only changing the number of hidden units in our hidden layer, the momentum and the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(784, int(params['hidden_units'])),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(int(params['hidden_units']), 10),\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=params['momentum'])\n",
    "    \n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = evaluate_one_epoch(model, dev_loader, criterion)\n",
    "    # We want to maximize the validation accuracy, so we return the negative of it \n",
    "    # because the hyperparameter optimizer minimizes the objective.\n",
    "    return {'loss': -val_acc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'hidden_units': hp.quniform('hidden_units', 50, 200, 10),\n",
    "    'lr': hp.loguniform('lr', -5, -1),\n",
    "    'momentum': hp.uniform('momentum', 0, 1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:09<00:00,  8.59s/trial, best loss: -0.97725]           \n",
      "Best hyperparameters found:  {'hidden_units': 200.0, 'lr': 0.14983990022656565, 'momentum': 0.6671579815534182}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(\"Best hyperparameters found: \", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with the hyperparameter above we have an accuracy of 0.977 in the validation set. We remember we were getting an accuracy of approximately 0.93 before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## <font color= #0cc754> 6. Convolutional Neural Network </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use Convolutional Neural Networks (CNNs) to do the same task. These networks have demonstrated great performance on many deep learning tasks, especially in computer vision. Therefore it is expected that this one will do a better job in recognizing the digits when compared to our previous Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the data again (we will need to reshape the data this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('./data/mnist_data.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "X_train, y_train = train_set\n",
    "X_valid, y_valid = valid_set\n",
    "X_test, y_test = test_set\n",
    "X_train=np.vstack((X_train,X_valid))\n",
    "y_train=np.hstack((y_train,y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " # We need to reshape the data into a 1x28x28 image (1 is due to being in greyscale), \n",
    " # since that's what our Convolutional Neural Network expects.\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, 28, 28))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our train data into dev (development or validation) and training data. Same as before we do a treatment making them into PyTorch tensors and batching them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_set, valid_set, test_set, X_valid, y_valid\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2, random_state=42, \n",
    "                                                 shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "dev_x_tensor = torch.tensor(X_dev, dtype=torch.float32)\n",
    "dev_y_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
    "test_x_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "test_y_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
    "dev_dataset = TensorDataset(dev_x_tensor, dev_y_tensor)\n",
    "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the model. What we want is:\n",
    "* A convolutional layer with 32 filters of size 3x3\n",
    "\n",
    "* A ReLU nonlinearity\n",
    "\n",
    "* A max pooling layer with size 2x2\n",
    "\n",
    "* A convolutional layer with 64 filters of size 3x3\n",
    "\n",
    "* A ReLU nonlinearity\n",
    "\n",
    "* A max pooling layer with size 2x2\n",
    "\n",
    "* A flatten layer\n",
    "\n",
    "* A fully connected layer with 128 neurons\n",
    "\n",
    "* A dropout layer with drop probability 0.5\n",
    "\n",
    "* A fully-connected layer with 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1, 32, (3, 3)),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d((2, 2)),\n",
    "          nn.Conv2d(32, 64, (3, 3)),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d((2, 2)),\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(1600, 128),\n",
    "          nn.Dropout(0.5),\n",
    "          nn.Linear(128, 10),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this kind of NN can take a while. My GPU is compatible with CUDA, which allows the training to use the GPU. I won't be using it here, but you can (after installing Pytorch with CUDA) see if it is available using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using the CPU for this example, but you can change it to use the GPU by changing the device variable to torch.device(\"cuda\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:13<01:59, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0212, Training Accuracy: 0.993 Validation Loss: 0.0354, Validation Accuracy: 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:25<01:41, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0190, Training Accuracy: 0.994 Validation Loss: 0.0406, Validation Accuracy: 0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:38<01:28, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0174, Training Accuracy: 0.994 Validation Loss: 0.0386, Validation Accuracy: 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:50<01:15, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0171, Training Accuracy: 0.994 Validation Loss: 0.0358, Validation Accuracy: 0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:02<01:02, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0139, Training Accuracy: 0.996 Validation Loss: 0.0476, Validation Accuracy: 0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:15<00:49, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0141, Training Accuracy: 0.995 Validation Loss: 0.0402, Validation Accuracy: 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:27<00:37, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0118, Training Accuracy: 0.996 Validation Loss: 0.0429, Validation Accuracy: 0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:40<00:24, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0114, Training Accuracy: 0.996 Validation Loss: 0.0517, Validation Accuracy: 0.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:52<00:12, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0118, Training Accuracy: 0.996 Validation Loss: 0.0413, Validation Accuracy: 0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:05<00:00, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0115, Training Accuracy: 0.996 Validation Loss: 0.0397, Validation Accuracy: 0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the device\n",
    "model.to(device)\n",
    "# We need to update the train_one_epoch and evaluate_one_epoch functions to move data to the device\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy of training\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    return running_loss / len(train_loader), correct_predictions / total_predictions\n",
    "\n",
    "def evaluate_one_epoch(model, dev_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dev_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy of validation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    return running_loss / len(dev_loader), correct_predictions / total_predictions\n",
    "\n",
    "# Training loop with device\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0)\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate_one_epoch(model, dev_loader, criterion, device)\n",
    "    print(f\"Training Loss: {train_loss[0]:.4f}, Training Accuracy: {train_loss[1]:.3f}\"\n",
    "          + f\" Validation Loss: {val_loss[0]:.4f}, Validation Accuracy: {val_loss[1]:.3f}\")\n",
    "\n",
    "# Save the model\n",
    "#torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see here that even without any hyperparameter we get an accuracy > 0.99, compared with the other neural network with 0.93 and 0.977 (with Hyperparameter Tuning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
